{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "#from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"M_CNN_M1_10_20.h5\"\n",
    "IMG_ROWS, IMG_COLS = 224, 224\n",
    "INPUT_SHAPE=(224, 224, 3)\n",
    "PATH = 'data/'\n",
    "TRAIN_DATA_PATH = os.path.join(PATH, 'Training')\n",
    "TEST_DATA_PATH = os.path.join(PATH, 'Testing')\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "CLASS_MODE = 'categorical'\n",
    "COLOR_MODE = 'rgb'\n",
    "SAVE_FORMAT = 'png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.DS_Store', 'glioma_tumor', 'meningioma_tumor', 'no_tumor',\n",
       "       'pituitary_tumor'], dtype='<U16')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir_list =np.sort(os.listdir(TRAIN_DATA_PATH))\n",
    "data_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir_list_Test =np.sort(os.listdir(TEST_DATA_PATH))\n",
    "data_dir_list_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    brightness_range=[0.6, 0.9],\n",
    "    zoom_range=[0.5,1.5],\n",
    "    rescale=1./255,\n",
    "    vertical_flip=True, \n",
    "    validation_split=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2297 images belonging to 4 classes.\n",
      "Found 573 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        TRAIN_DATA_PATH,\n",
    "        target_size=(IMG_ROWS, IMG_COLS), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE, \n",
    "        shuffle=True,   \n",
    "        save_format=SAVE_FORMAT, \n",
    "        subset=\"training\")\n",
    "\n",
    "\n",
    "val_generator = train_data_gen.flow_from_directory(\n",
    "    TRAIN_DATA_PATH,\n",
    "    target_size=(IMG_ROWS, IMG_COLS), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE,\n",
    "    color_mode=COLOR_MODE, \n",
    "    shuffle=True,   \n",
    "    save_format=SAVE_FORMAT, \n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glioma_tumor': 0, 'meningioma_tumor': 1, 'no_tumor': 2, 'pituitary_tumor': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_PATH,\n",
    "    target_size=(IMG_ROWS, IMG_COLS),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE,\n",
    "    color_mode=COLOR_MODE, \n",
    "    shuffle = False,\n",
    "    seed=None,  \n",
    "    save_format=SAVE_FORMAT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_nasnet_model():\n",
    "    \n",
    "    # Base model, with weights pre-trained on ImageNet.\n",
    "    base_model = NASNetMobile(INPUT_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.8\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=False)\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = optimizer,\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "lr_scheduler =keras.callbacks.LearningRateScheduler(step_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks_list  = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# labels_dict : {ind_label: count_label}\n",
    "# mu : parameter to tune \n",
    "\n",
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = sum(labels_dict.values())\n",
    "    keys = labels_dict.keys()\n",
    "    class_weight = dict()\n",
    "    for key in keys:\n",
    "        score = (mu*total)/float(labels_dict[key])\n",
    "        class_weight[key] = score if score > 1.0 else 1.0\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "# random labels_dict\n",
    "labels_dict = {0: 826, 1: 822, 2: 395, 3: 827}\n",
    "\n",
    "class_wt = create_class_weight(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 1/100\n",
      "36/36 [==============================] - 98s 3s/step - loss: 3.1123 - accuracy: 0.4763 - val_loss: 1.7293 - val_accuracy: 0.5079\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.9062 - accuracy: 0.5825 - val_loss: 1.2432 - val_accuracy: 0.6003\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 2.0090 - accuracy: 0.5816 - val_loss: 1.9909 - val_accuracy: 0.5236\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 2.0124 - accuracy: 0.5825 - val_loss: 1.5188 - val_accuracy: 0.6108\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 2.2965 - accuracy: 0.5777 - val_loss: 3.1063 - val_accuracy: 0.4223\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 2.1866 - accuracy: 0.5977 - val_loss: 1.2587 - val_accuracy: 0.6719\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.8761 - accuracy: 0.6313 - val_loss: 1.6404 - val_accuracy: 0.5812\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.9234 - accuracy: 0.6025 - val_loss: 1.5199 - val_accuracy: 0.5986\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 2.1618 - accuracy: 0.6021 - val_loss: 1.3994 - val_accuracy: 0.6475\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 1.5871 - accuracy: 0.6491 - val_loss: 1.1375 - val_accuracy: 0.6370\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.5163 - accuracy: 0.6430 - val_loss: 1.0626 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.4522 - accuracy: 0.6356 - val_loss: 0.8492 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 1.1972 - accuracy: 0.6739 - val_loss: 0.9202 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.2799 - accuracy: 0.6495 - val_loss: 1.1452 - val_accuracy: 0.6283\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.1652 - accuracy: 0.6643 - val_loss: 0.9141 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 1.2605 - accuracy: 0.6465 - val_loss: 0.9525 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.1278 - accuracy: 0.6761 - val_loss: 1.0240 - val_accuracy: 0.6318\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 1.1127 - accuracy: 0.6591 - val_loss: 0.8947 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 1.2102 - accuracy: 0.6408 - val_loss: 1.4663 - val_accuracy: 0.5462\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 1.0510 - accuracy: 0.6657 - val_loss: 0.9622 - val_accuracy: 0.6440\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 1.0058 - accuracy: 0.6791 - val_loss: 0.8377 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.9402 - accuracy: 0.6857 - val_loss: 0.8029 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.8753 - accuracy: 0.7000 - val_loss: 0.7550 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.9243 - accuracy: 0.6800 - val_loss: 0.8297 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 0.9203 - accuracy: 0.6896 - val_loss: 0.7737 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 0.8691 - accuracy: 0.6887 - val_loss: 0.8573 - val_accuracy: 0.6649\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.8413 - accuracy: 0.7044 - val_loss: 0.8167 - val_accuracy: 0.6702\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 0.8734 - accuracy: 0.6835 - val_loss: 0.7949 - val_accuracy: 0.6771\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 102s 3s/step - loss: 0.8566 - accuracy: 0.6983 - val_loss: 0.8695 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 98s 3s/step - loss: 0.8162 - accuracy: 0.7027 - val_loss: 0.8104 - val_accuracy: 0.6736\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 97s 3s/step - loss: 0.7781 - accuracy: 0.7209 - val_loss: 0.7156 - val_accuracy: 0.6789\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 107s 3s/step - loss: 0.8078 - accuracy: 0.6931 - val_loss: 0.8146 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 102s 3s/step - loss: 0.7902 - accuracy: 0.6948 - val_loss: 0.7703 - val_accuracy: 0.6824\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 108s 3s/step - loss: 0.7960 - accuracy: 0.7118 - val_loss: 0.8564 - val_accuracy: 0.6510\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 107s 3s/step - loss: 0.7706 - accuracy: 0.7148 - val_loss: 0.8077 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 110s 3s/step - loss: 0.7922 - accuracy: 0.6979 - val_loss: 0.7453 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 100s 3s/step - loss: 0.8013 - accuracy: 0.6896 - val_loss: 0.7943 - val_accuracy: 0.6632\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 100s 3s/step - loss: 0.7883 - accuracy: 0.7118 - val_loss: 0.7817 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 100s 3s/step - loss: 0.7780 - accuracy: 0.7057 - val_loss: 0.7455 - val_accuracy: 0.6841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 98s 3s/step - loss: 0.7730 - accuracy: 0.7079 - val_loss: 0.7743 - val_accuracy: 0.6806\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 98s 3s/step - loss: 0.7400 - accuracy: 0.7088 - val_loss: 0.7163 - val_accuracy: 0.7225\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 99s 3s/step - loss: 0.7470 - accuracy: 0.7162 - val_loss: 0.6929 - val_accuracy: 0.7208\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 99s 3s/step - loss: 0.7478 - accuracy: 0.7162 - val_loss: 0.7569 - val_accuracy: 0.6824\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 100s 3s/step - loss: 0.7245 - accuracy: 0.7244 - val_loss: 0.6985 - val_accuracy: 0.7312\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 102s 3s/step - loss: 0.7035 - accuracy: 0.7266 - val_loss: 0.7083 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 46/100\n",
      "36/36 [==============================] - 102s 3s/step - loss: 0.7450 - accuracy: 0.7101 - val_loss: 0.7015 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 47/100\n",
      "36/36 [==============================] - 110s 3s/step - loss: 0.7287 - accuracy: 0.7270 - val_loss: 0.7547 - val_accuracy: 0.6719\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 48/100\n",
      "36/36 [==============================] - 113s 3s/step - loss: 0.7375 - accuracy: 0.7127 - val_loss: 0.7003 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 49/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7030 - accuracy: 0.7253 - val_loss: 0.6784 - val_accuracy: 0.7312\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 50/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7520 - accuracy: 0.7096 - val_loss: 0.7329 - val_accuracy: 0.7208\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 51/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7059 - accuracy: 0.7336 - val_loss: 0.7095 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 52/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6805 - accuracy: 0.7314 - val_loss: 0.7290 - val_accuracy: 0.6789\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 53/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7380 - accuracy: 0.7122 - val_loss: 0.7607 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 54/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7015 - accuracy: 0.7279 - val_loss: 0.7210 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 55/100\n",
      "36/36 [==============================] - 94s 3s/step - loss: 0.7125 - accuracy: 0.7175 - val_loss: 0.7447 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 56/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6988 - accuracy: 0.7362 - val_loss: 0.7389 - val_accuracy: 0.6824\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 57/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7088 - accuracy: 0.7344 - val_loss: 0.7530 - val_accuracy: 0.6649\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 58/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7026 - accuracy: 0.7305 - val_loss: 0.6975 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 59/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6944 - accuracy: 0.7275 - val_loss: 0.7643 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 60/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6881 - accuracy: 0.7288 - val_loss: 0.7381 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 61/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7064 - accuracy: 0.7296 - val_loss: 0.7305 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 62/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6926 - accuracy: 0.7201 - val_loss: 0.6573 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 63/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7114 - accuracy: 0.7266 - val_loss: 0.6621 - val_accuracy: 0.7295\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 64/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6987 - accuracy: 0.7310 - val_loss: 0.7073 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 65/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 0.6885 - accuracy: 0.7384 - val_loss: 0.7273 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 66/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7123 - accuracy: 0.7249 - val_loss: 0.7691 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 67/100\n",
      "36/36 [==============================] - 96s 3s/step - loss: 0.6938 - accuracy: 0.7344 - val_loss: 0.6973 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 68/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6959 - accuracy: 0.7336 - val_loss: 0.6838 - val_accuracy: 0.7260\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 69/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7099 - accuracy: 0.7253 - val_loss: 0.7040 - val_accuracy: 0.7260\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 70/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6476 - accuracy: 0.7527 - val_loss: 0.7165 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 71/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6914 - accuracy: 0.7327 - val_loss: 0.6831 - val_accuracy: 0.7382\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 72/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.6794 - accuracy: 0.7296 - val_loss: 0.7099 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 73/100\n",
      "36/36 [==============================] - 95s 3s/step - loss: 0.7009 - accuracy: 0.7236 - val_loss: 0.7052 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 74/100\n",
      "36/36 [==============================] - 100s 3s/step - loss: 0.6858 - accuracy: 0.7340 - val_loss: 0.6523 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 75/100\n",
      "36/36 [==============================] - 117s 3s/step - loss: 0.6848 - accuracy: 0.7209 - val_loss: 0.7133 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 76/100\n",
      "36/36 [==============================] - 106s 3s/step - loss: 0.7072 - accuracy: 0.7318 - val_loss: 0.7124 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 77/100\n",
      "36/36 [==============================] - 112s 3s/step - loss: 0.6611 - accuracy: 0.7327 - val_loss: 0.7232 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 116s 3s/step - loss: 0.6912 - accuracy: 0.7340 - val_loss: 0.7334 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 79/100\n",
      "36/36 [==============================] - 109s 3s/step - loss: 0.7267 - accuracy: 0.7127 - val_loss: 0.7098 - val_accuracy: 0.6928\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 80/100\n",
      "36/36 [==============================] - 115s 3s/step - loss: 0.6955 - accuracy: 0.7266 - val_loss: 0.7169 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 81/100\n",
      "36/36 [==============================] - 106s 3s/step - loss: 0.7022 - accuracy: 0.7270 - val_loss: 0.6952 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 82/100\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.7027 - accuracy: 0.7179"
     ]
    }
   ],
   "source": [
    "model = create_nasnet_model()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    workers=6,\n",
    "    max_queue_size=100,\n",
    "    verbose=True,\n",
    "    callbacks=callbacks_list,\n",
    "    #class_weight = class_wt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211) \n",
    "plt.plot(history.history['accuracy']) \n",
    "plt.plot(history.history['val_accuracy']) \n",
    "plt.title('model accuracy') \n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Accuracy', 'val_acc'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(212) \n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout() \n",
    "plt.savefig('acc_loss_50.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
