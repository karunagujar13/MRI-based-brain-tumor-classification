{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Uses Inception "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses :\n",
    "    1. Inception as the base architecture \n",
    "    2. Batch size =32 and 150 epochs. \n",
    "    3. Uses processed (Histogram equalized) data \n",
    "    4. Without balanced weights.\n",
    "    5. Uses LR scheduler step decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "#from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"M_CNN_M1_10_20.h5\"\n",
    "IMG_ROWS, IMG_COLS = 150, 150\n",
    "INPUT_SHAPE=(150, 150, 3)\n",
    "PATH = 'data/processed_data/'\n",
    "TRAIN_DATA_PATH = os.path.join(PATH, 'Training')\n",
    "TEST_DATA_PATH = os.path.join(PATH, 'Testing')\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 150\n",
    "CLASS_MODE = 'categorical'\n",
    "COLOR_MODE = 'rgb'\n",
    "SAVE_FORMAT = 'png'\n",
    "#32 bs, 50 e, processed data - 75%\n",
    "#32 bs, 50 e, raw data - 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir_list =np.sort(os.listdir(TRAIN_DATA_PATH))\n",
    "data_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir_list_Test =np.sort(os.listdir(TEST_DATA_PATH))\n",
    "data_dir_list_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.5, \n",
    "    rescale=1./255,\n",
    "    vertical_flip=True, \n",
    "    validation_split=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2297 images belonging to 4 classes.\n",
      "Found 573 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        TRAIN_DATA_PATH,\n",
    "        target_size=(IMG_ROWS, IMG_COLS), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE, \n",
    "        shuffle=True,   \n",
    "        save_format=SAVE_FORMAT, \n",
    "        subset=\"training\")\n",
    "\n",
    "\n",
    "val_generator = train_data_gen.flow_from_directory(\n",
    "    TRAIN_DATA_PATH,\n",
    "    target_size=(IMG_ROWS, IMG_COLS), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE,\n",
    "    color_mode=COLOR_MODE, \n",
    "    shuffle=True,   \n",
    "    save_format=SAVE_FORMAT, \n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glioma_tumor': 0, 'meningioma_tumor': 1, 'no_tumor': 2, 'pituitary_tumor': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_PATH,\n",
    "    target_size=(IMG_ROWS, IMG_COLS),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE,\n",
    "    color_mode=COLOR_MODE, \n",
    "    shuffle = False,\n",
    "    seed=None,  \n",
    "    save_format=SAVE_FORMAT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_inception_model():\n",
    "    \n",
    "    # Base model, with weights pre-trained on ImageNet.\n",
    "    base_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(base_model.input, x)\n",
    "    \n",
    "    learning_rate = 0.5\n",
    "    momentum = 0.8\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=False)\n",
    "\n",
    "    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.5\n",
    "   drop = 0.5\n",
    "   epochs_drop = 20.0\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "lr_scheduler =keras.callbacks.LearningRateScheduler(step_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks_list  = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 1/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 9353.8232 - acc: 0.2778 - val_loss: 1.4740 - val_acc: 0.2897\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 2/150\n",
      "72/72 [==============================] - 37s 514ms/step - loss: 1.4734 - acc: 0.2912 - val_loss: 1.5112 - val_acc: 0.2845\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 3/150\n",
      "72/72 [==============================] - 37s 512ms/step - loss: 1.3590 - acc: 0.2908 - val_loss: 1.3522 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 4/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.4721 - acc: 0.2904 - val_loss: 1.3508 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 5/150\n",
      "72/72 [==============================] - 38s 523ms/step - loss: 1.9549 - acc: 0.2982 - val_loss: 1.3573 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 6/150\n",
      "72/72 [==============================] - 38s 525ms/step - loss: 6.9336 - acc: 0.2943 - val_loss: 1.3797 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 7/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 207.9568 - acc: 0.2743 - val_loss: 1.5007 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 8/150\n",
      "72/72 [==============================] - 38s 528ms/step - loss: 1.3764 - acc: 0.2656 - val_loss: 1.3633 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 9/150\n",
      "72/72 [==============================] - 38s 525ms/step - loss: 1.3677 - acc: 0.2660 - val_loss: 1.3635 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 10/150\n",
      "72/72 [==============================] - 38s 525ms/step - loss: 1.3656 - acc: 0.2886 - val_loss: 1.3600 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 11/150\n",
      "72/72 [==============================] - 38s 526ms/step - loss: 1.3640 - acc: 0.2734 - val_loss: 1.3491 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 12/150\n",
      "72/72 [==============================] - 38s 521ms/step - loss: 1.3573 - acc: 0.3030 - val_loss: 1.3651 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 13/150\n",
      "72/72 [==============================] - 38s 527ms/step - loss: 1.3681 - acc: 0.2773 - val_loss: 1.3597 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 14/150\n",
      "72/72 [==============================] - 38s 528ms/step - loss: 1.3631 - acc: 0.2943 - val_loss: 1.3582 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 15/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 1.3677 - acc: 0.2934 - val_loss: 1.3486 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 16/150\n",
      "72/72 [==============================] - 39s 536ms/step - loss: 1.3602 - acc: 0.2821 - val_loss: 1.3684 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 17/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3658 - acc: 0.2825 - val_loss: 13.3250 - val_acc: 0.3560\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 18/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.5302 - acc: 0.2843 - val_loss: 1.3547 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.5.\n",
      "Epoch 19/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3617 - acc: 0.2878 - val_loss: 1.3563 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 20/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3581 - acc: 0.2738 - val_loss: 1.3524 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 21/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3569 - acc: 0.2756 - val_loss: 1.3486 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 22/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3544 - acc: 0.2886 - val_loss: 1.3572 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 23/150\n",
      "72/72 [==============================] - 37s 521ms/step - loss: 1.3528 - acc: 0.2982 - val_loss: 1.3491 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 24/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3587 - acc: 0.2725 - val_loss: 1.3530 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 25/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3530 - acc: 0.2917 - val_loss: 1.3571 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 26/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3552 - acc: 0.2847 - val_loss: 1.3594 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 27/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3559 - acc: 0.2917 - val_loss: 1.3530 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 28/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3547 - acc: 0.2895 - val_loss: 1.3607 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 29/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3558 - acc: 0.2904 - val_loss: 1.3541 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 30/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3592 - acc: 0.3008 - val_loss: 1.3608 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 31/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3521 - acc: 0.2978 - val_loss: 1.3680 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 32/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3585 - acc: 0.2704 - val_loss: 1.3627 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 33/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3564 - acc: 0.2860 - val_loss: 1.3540 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 34/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3567 - acc: 0.2764 - val_loss: 1.3542 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 35/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3575 - acc: 0.2808 - val_loss: 1.3521 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 36/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3543 - acc: 0.2830 - val_loss: 1.3501 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 37/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3556 - acc: 0.2882 - val_loss: 1.3555 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 38/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3570 - acc: 0.2677 - val_loss: 1.3497 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.25.\n",
      "Epoch 39/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3574 - acc: 0.2882 - val_loss: 1.3584 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 40/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3533 - acc: 0.2704 - val_loss: 1.3496 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 41/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3528 - acc: 0.2860 - val_loss: 1.3489 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 42/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3536 - acc: 0.2921 - val_loss: 1.3489 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 43/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3524 - acc: 0.2930 - val_loss: 1.3491 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 44/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3518 - acc: 0.2860 - val_loss: 1.3547 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 45/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3521 - acc: 0.2838 - val_loss: 1.3502 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 46/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3518 - acc: 0.2782 - val_loss: 1.3489 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 47/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3517 - acc: 0.2886 - val_loss: 1.3505 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 48/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3534 - acc: 0.2751 - val_loss: 1.3493 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 49/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3523 - acc: 0.2791 - val_loss: 1.3502 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 50/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3501 - acc: 0.3108 - val_loss: 1.3528 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 51/150\n",
      "72/72 [==============================] - 37s 514ms/step - loss: 1.3515 - acc: 0.2791 - val_loss: 1.3495 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 52/150\n",
      "72/72 [==============================] - 37s 514ms/step - loss: 1.3535 - acc: 0.2899 - val_loss: 1.3518 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 53/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3514 - acc: 0.2817 - val_loss: 1.3486 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 54/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3505 - acc: 0.3021 - val_loss: 1.3493 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 55/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3533 - acc: 0.2782 - val_loss: 1.3507 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 56/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3527 - acc: 0.2843 - val_loss: 1.3515 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 57/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3521 - acc: 0.2939 - val_loss: 1.3505 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 58/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3520 - acc: 0.2838 - val_loss: 1.3538 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.125.\n",
      "Epoch 59/150\n",
      "72/72 [==============================] - 37s 514ms/step - loss: 1.3516 - acc: 0.2821 - val_loss: 1.3484 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 60/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3497 - acc: 0.2699 - val_loss: 1.3488 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 61/150\n",
      "72/72 [==============================] - 38s 521ms/step - loss: 1.3506 - acc: 0.2769 - val_loss: 1.3497 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 62/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3511 - acc: 0.2638 - val_loss: 1.3493 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 63/150\n",
      "72/72 [==============================] - 38s 523ms/step - loss: 1.3499 - acc: 0.2895 - val_loss: 1.3489 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 64/150\n",
      "72/72 [==============================] - 38s 525ms/step - loss: 1.3500 - acc: 0.2808 - val_loss: 1.3496 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 65/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3503 - acc: 0.2830 - val_loss: 1.3490 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 66/150\n",
      "72/72 [==============================] - 38s 522ms/step - loss: 1.3507 - acc: 0.2669 - val_loss: 1.3492 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 67/150\n",
      "72/72 [==============================] - 38s 521ms/step - loss: 1.3501 - acc: 0.2769 - val_loss: 1.3496 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 68/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 1.3506 - acc: 0.2856 - val_loss: 1.3491 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 69/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 1.3491 - acc: 0.2926 - val_loss: 1.3502 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 70/150\n",
      "72/72 [==============================] - 38s 523ms/step - loss: 1.3502 - acc: 0.2830 - val_loss: 1.3490 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 71/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3491 - acc: 0.2908 - val_loss: 1.3500 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 72/150\n",
      "72/72 [==============================] - 38s 522ms/step - loss: 1.3504 - acc: 0.2865 - val_loss: 1.3490 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 73/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3500 - acc: 0.2782 - val_loss: 1.3491 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 74/150\n",
      "72/72 [==============================] - 38s 523ms/step - loss: 1.3515 - acc: 0.2712 - val_loss: 1.3484 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 75/150\n",
      "72/72 [==============================] - 38s 525ms/step - loss: 1.3505 - acc: 0.2825 - val_loss: 1.3489 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 76/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3514 - acc: 0.2817 - val_loss: 1.3483 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 77/150\n",
      "72/72 [==============================] - 38s 523ms/step - loss: 1.3495 - acc: 0.2808 - val_loss: 1.3489 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 78/150\n",
      "72/72 [==============================] - 38s 523ms/step - loss: 1.3486 - acc: 0.2991 - val_loss: 1.3518 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0625.\n",
      "Epoch 79/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3502 - acc: 0.2912 - val_loss: 1.3509 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 80/150\n",
      "72/72 [==============================] - 38s 522ms/step - loss: 1.3497 - acc: 0.2886 - val_loss: 1.3483 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 81/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3491 - acc: 0.2956 - val_loss: 1.3486 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 82/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3491 - acc: 0.2991 - val_loss: 1.3492 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 83/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3489 - acc: 0.2843 - val_loss: 1.3485 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 84/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3492 - acc: 0.2721 - val_loss: 1.3481 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 85/150\n",
      "72/72 [==============================] - 38s 526ms/step - loss: 1.3486 - acc: 0.2778 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 86/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3520 - acc: 0.2930 - val_loss: 1.3488 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 87/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3494 - acc: 0.2738 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 88/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3497 - acc: 0.2682 - val_loss: 1.3483 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 89/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3490 - acc: 0.2764 - val_loss: 1.3485 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 90/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3490 - acc: 0.2804 - val_loss: 1.3487 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 91/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3494 - acc: 0.2960 - val_loss: 1.3486 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 92/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3494 - acc: 0.2743 - val_loss: 1.3485 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 93/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3492 - acc: 0.2882 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 94/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3492 - acc: 0.2821 - val_loss: 1.3488 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 95/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3498 - acc: 0.2738 - val_loss: 1.3487 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 96/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3498 - acc: 0.2799 - val_loss: 1.3486 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 97/150\n",
      "72/72 [==============================] - 38s 521ms/step - loss: 1.3493 - acc: 0.2625 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 98/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3494 - acc: 0.2769 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.03125.\n",
      "Epoch 99/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3496 - acc: 0.2734 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 100/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3490 - acc: 0.2821 - val_loss: 1.3484 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 101/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3488 - acc: 0.2921 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 102/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3485 - acc: 0.2747 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 103/150\n",
      "72/72 [==============================] - 38s 522ms/step - loss: 1.3487 - acc: 0.2721 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 104/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3487 - acc: 0.2712 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 105/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3488 - acc: 0.2869 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 106/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3488 - acc: 0.2799 - val_loss: 1.3482 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 107/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3485 - acc: 0.2756 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 108/150\n",
      "72/72 [==============================] - 38s 522ms/step - loss: 1.3486 - acc: 0.2852 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 109/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 1.3486 - acc: 0.2882 - val_loss: 1.3482 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 110/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3489 - acc: 0.2825 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 111/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3485 - acc: 0.2634 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 112/150\n",
      "72/72 [==============================] - 38s 522ms/step - loss: 1.3486 - acc: 0.2873 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 113/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3488 - acc: 0.2843 - val_loss: 1.3484 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 114/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3490 - acc: 0.2804 - val_loss: 1.3484 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 115/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3489 - acc: 0.2751 - val_loss: 1.3483 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 116/150\n",
      "72/72 [==============================] - 38s 526ms/step - loss: 1.3487 - acc: 0.2773 - val_loss: 1.3484 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 117/150\n",
      "72/72 [==============================] - 38s 521ms/step - loss: 1.3489 - acc: 0.2812 - val_loss: 1.3483 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 118/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3488 - acc: 0.2725 - val_loss: 1.3483 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.015625.\n",
      "Epoch 119/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3486 - acc: 0.2865 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 120/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3484 - acc: 0.2738 - val_loss: 1.3483 - val_acc: 0.2880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 121/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3483 - acc: 0.2873 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 122/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3483 - acc: 0.2743 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 123/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3484 - acc: 0.2773 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 124/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3483 - acc: 0.2804 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 125/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3484 - acc: 0.2704 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 126/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3487 - acc: 0.2682 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 127/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 1.3483 - acc: 0.2834 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 128/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3484 - acc: 0.2664 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 129/150\n",
      "72/72 [==============================] - 38s 521ms/step - loss: 1.3483 - acc: 0.2760 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 130/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3484 - acc: 0.2825 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 131/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 1.3484 - acc: 0.2799 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 132/150\n",
      "72/72 [==============================] - 38s 525ms/step - loss: 1.3484 - acc: 0.2721 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 133/150\n",
      "72/72 [==============================] - 37s 514ms/step - loss: 1.3484 - acc: 0.2795 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 134/150\n",
      "72/72 [==============================] - 37s 521ms/step - loss: 1.3484 - acc: 0.2743 - val_loss: 1.3482 - val_acc: 0.2862\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 135/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3485 - acc: 0.2599 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 136/150\n",
      "72/72 [==============================] - 37s 518ms/step - loss: 1.3484 - acc: 0.2808 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 137/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3483 - acc: 0.2821 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 138/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3483 - acc: 0.2760 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.0078125.\n",
      "Epoch 139/150\n",
      "72/72 [==============================] - 38s 524ms/step - loss: 1.3484 - acc: 0.2795 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 140/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3481 - acc: 0.2838 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 141/150\n",
      "72/72 [==============================] - 37s 521ms/step - loss: 1.3482 - acc: 0.2830 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 142/150\n",
      "72/72 [==============================] - 38s 521ms/step - loss: 1.3482 - acc: 0.2821 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 143/150\n",
      "72/72 [==============================] - 37s 520ms/step - loss: 1.3482 - acc: 0.2760 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 144/150\n",
      "72/72 [==============================] - 38s 525ms/step - loss: 1.3482 - acc: 0.2838 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 145/150\n",
      "72/72 [==============================] - 37s 516ms/step - loss: 1.3481 - acc: 0.2869 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 146/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3482 - acc: 0.2830 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 147/150\n",
      "72/72 [==============================] - 37s 519ms/step - loss: 1.3482 - acc: 0.2730 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 148/150\n",
      "72/72 [==============================] - 37s 517ms/step - loss: 1.3482 - acc: 0.2690 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 149/150\n",
      "72/72 [==============================] - 38s 523ms/step - loss: 1.3482 - acc: 0.2830 - val_loss: 1.3482 - val_acc: 0.2880\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.00390625.\n",
      "Epoch 150/150\n",
      "72/72 [==============================] - 37s 515ms/step - loss: 1.3482 - acc: 0.2812 - val_loss: 1.3482 - val_acc: 0.2880\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "   workers=6,\n",
    "    max_queue_size=100,\n",
    "    verbose=True,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 150, 150, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 74, 74, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 74, 74, 32)   96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 74, 74, 32)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 72, 72, 32)   9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 72, 72, 32)   96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 72, 72, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 72, 72, 64)   18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 72, 72, 64)   192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 72, 72, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 35, 35, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 35, 35, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 35, 35, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 35, 35, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 33, 33, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 33, 33, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 33, 33, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_27 (Conv2D)              (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 16, 16, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 16, 16, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 7, 96)     82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 7, 7, 384)    1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 7, 7, 96)     288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 7, 7, 384)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 7, 7, 96)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 7, 7, 128)    384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 7, 7, 128)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 7, 7, 128)    114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 7, 7, 128)    384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 7, 7, 128)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 7, 7, 128)    114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 7, 7, 128)    384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 7, 7, 128)    384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 7, 7, 128)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 7, 7, 128)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 7, 7, 128)    114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 7, 7, 128)    114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 7, 7, 128)    384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 7, 7, 128)    384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 7, 7, 128)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 7, 7, 128)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 7, 7, 192)    172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 7, 7, 192)    172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 7, 7, 192)    576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 7, 7, 192)    576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 7, 7, 192)    576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 7, 7, 192)    576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 7, 7, 192)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 7, 7, 192)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 7, 7, 192)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 7, 7, 192)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 7, 7, 160)    480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 160)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 7, 7, 160)    179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 160)    480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 160)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 7, 7, 160)    179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 7, 7, 160)    480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 7, 7, 160)    480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 160)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 160)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 7, 7, 160)    179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 7, 7, 160)    179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 7, 7, 160)    480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 7, 7, 160)    480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 160)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 160)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 7, 7, 192)    215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 7, 7, 192)    215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 7, 7, 192)    576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 7, 7, 192)    576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 7, 7, 192)    576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 7, 7, 192)    576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 192)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 192)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 192)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 7, 7, 192)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 7, 7, 160)    480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 7, 7, 160)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 7, 7, 160)    179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 7, 7, 160)    480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 7, 7, 160)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 7, 7, 160)    179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 7, 7, 160)    480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 7, 7, 160)    480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 7, 7, 160)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 7, 7, 160)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 7, 7, 160)    179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 7, 7, 160)    179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 7, 7, 160)    480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 7, 7, 160)    480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 7, 7, 160)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 7, 7, 160)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 7, 7, 192)    215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 7, 7, 192)    215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 7, 7, 192)    576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 7, 7, 192)    576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 7, 7, 192)    576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 7, 7, 192)    576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 7, 7, 192)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 7, 7, 192)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 7, 7, 192)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 7, 7, 192)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 7, 7, 192)    576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 7, 7, 192)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 7, 7, 192)    258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 7, 7, 192)    576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 7, 7, 192)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 7, 7, 192)    258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 7, 7, 192)    576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 7, 7, 192)    576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 7, 7, 192)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 7, 7, 192)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 7, 7, 192)    258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 7, 7, 192)    258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 7, 7, 192)    576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 7, 7, 192)    576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 7, 192)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 7, 7, 192)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 7, 7, 192)    258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 7, 7, 192)    258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 7, 7, 192)    576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 7, 7, 192)    576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 7, 7, 192)    576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 7, 7, 192)    576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 7, 7, 192)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 7, 7, 192)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 7, 7, 192)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 7, 7, 192)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 7, 7, 192)    576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 7, 7, 192)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 7, 7, 192)    258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 7, 7, 192)    576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 7, 7, 192)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 7, 7, 192)    258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 7, 7, 192)    576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 7, 7, 192)    576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 7, 7, 192)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 7, 7, 192)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 3, 3, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 3, 3, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 3, 3, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 3, 3, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 3, 3, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 3, 3, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 3, 3, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 3, 3, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 3, 3, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 3, 3, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_81 (BatchNo (None, 3, 3, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 3, 3, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 3, 3, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 3, 3, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 3, 3, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 3, 3, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 3, 3, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 3, 3, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 3, 3, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 3, 3, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 3, 3, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 3, 3, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 3, 3, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 3, 3, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 3, 3, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 3, 3, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 3, 3, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3, 3, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 3, 3, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 3, 3, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 3, 3, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 3, 3, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 3, 3, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 3, 3, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 3, 3, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 3, 3, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 3, 3, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 3, 3, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 3, 3, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 3, 3, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 3, 3, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 3, 3, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 3, 3, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 3, 3, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 3, 3, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 3, 3, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 3, 3, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 3, 3, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 3, 3, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 3, 3, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 3, 3, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 18432)        0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         18875392    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            4100        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 40,682,276\n",
      "Trainable params: 18,879,492\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 4s 302ms/step - loss: 1.4429 - acc: 0.2538\n",
      "Test Accuracy: 25.380709767341614%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
