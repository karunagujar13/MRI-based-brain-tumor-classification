{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "#from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, GlobalAveragePooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"M_CNN_M1_10_20.h5\"\n",
    "IMG_ROWS, IMG_COLS = 224, 224\n",
    "INPUT_SHAPE=(224, 224, 3)\n",
    "PATH = 'data/processed_data/'\n",
    "TRAIN_DATA_PATH = os.path.join(PATH, 'Training')\n",
    "TEST_DATA_PATH = os.path.join(PATH, 'Testing')\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 250\n",
    "CLASS_MODE = 'categorical'\n",
    "COLOR_MODE = 'rgb'\n",
    "SAVE_FORMAT = 'png'\n",
    "#40, 100 - 61\n",
    "#64 batch size, 150 epoch ---62.43\n",
    "#128, 150 -- 60.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir_list =np.sort(os.listdir(TRAIN_DATA_PATH))\n",
    "data_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir_list_Test =np.sort(os.listdir(TEST_DATA_PATH))\n",
    "data_dir_list_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "    rotation_range=50,\n",
    "    brightness_range=[0.2,0.8],\n",
    "    vertical_flip=True, \n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    zoom_range=[0.5,0.8],\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2297 images belonging to 4 classes.\n",
      "Found 573 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "        TRAIN_DATA_PATH,\n",
    "        target_size=(IMG_ROWS, IMG_COLS), \n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=CLASS_MODE,\n",
    "        color_mode=COLOR_MODE, \n",
    "        shuffle=True,   \n",
    "        save_format=SAVE_FORMAT, \n",
    "        subset=\"training\")\n",
    "\n",
    "\n",
    "val_generator = train_data_gen.flow_from_directory(\n",
    "    TRAIN_DATA_PATH,\n",
    "    target_size=(IMG_ROWS, IMG_COLS), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE,\n",
    "    color_mode=COLOR_MODE, \n",
    "    shuffle=True,   \n",
    "    save_format=SAVE_FORMAT, \n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2297"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glioma_tumor': 0, 'meningioma_tumor': 1, 'no_tumor': 2, 'pituitary_tumor': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 394 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DATA_PATH,\n",
    "    target_size=(IMG_ROWS, IMG_COLS),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=CLASS_MODE,\n",
    "    color_mode=COLOR_MODE, \n",
    "    shuffle = False,\n",
    "    seed=None,  \n",
    "    save_format=SAVE_FORMAT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_nasnet_model():\n",
    "    \n",
    "    # Base model, with weights pre-trained on ImageNet.\n",
    "    base_model = NASNetMobile(INPUT_SHAPE, weights='imagenet', include_top=False)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.8\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, nesterov=False)\n",
    "\n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = optimizer,\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "lr_scheduler =keras.callbacks.LearningRateScheduler(step_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks_list  = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 1/250\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.6714 - accuracy: 0.5054 - val_loss: 1.2479 - val_accuracy: 0.6265\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 2/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 1.2270 - accuracy: 0.6356 - val_loss: 1.1907 - val_accuracy: 0.5550\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 3/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 1.2206 - accuracy: 0.6482 - val_loss: 1.5857 - val_accuracy: 0.5602\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 4/250\n",
      "18/18 [==============================] - 94s 5s/step - loss: 1.1435 - accuracy: 0.6731 - val_loss: 0.9653 - val_accuracy: 0.6649\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 5/250\n",
      "18/18 [==============================] - 90s 5s/step - loss: 0.9893 - accuracy: 0.6983 - val_loss: 0.9000 - val_accuracy: 0.6771\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 6/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.9540 - accuracy: 0.6992 - val_loss: 1.0394 - val_accuracy: 0.6370\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 7/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 1.0087 - accuracy: 0.6717 - val_loss: 1.2376 - val_accuracy: 0.5934\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 8/250\n",
      "18/18 [==============================] - 91s 5s/step - loss: 1.0089 - accuracy: 0.6870 - val_loss: 1.6908 - val_accuracy: 0.4764\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 9/250\n",
      "18/18 [==============================] - 87s 5s/step - loss: 0.9405 - accuracy: 0.6996 - val_loss: 0.8651 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 10/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.8753 - accuracy: 0.7175 - val_loss: 1.1118 - val_accuracy: 0.5934\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 11/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.7537 - accuracy: 0.7327 - val_loss: 0.7179 - val_accuracy: 0.7330\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 12/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.7418 - accuracy: 0.7379 - val_loss: 0.8053 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 13/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.7653 - accuracy: 0.7310 - val_loss: 0.9694 - val_accuracy: 0.6335\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 14/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.7395 - accuracy: 0.7375 - val_loss: 0.7885 - val_accuracy: 0.6667\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 15/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.7433 - accuracy: 0.7266 - val_loss: 0.7454 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 16/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.7174 - accuracy: 0.7240 - val_loss: 0.7394 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 17/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.7049 - accuracy: 0.7453 - val_loss: 0.7662 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 18/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.6848 - accuracy: 0.7431 - val_loss: 0.7117 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.05.\n",
      "Epoch 19/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.7231 - accuracy: 0.7301 - val_loss: 0.8877 - val_accuracy: 0.6021\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 20/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.6754 - accuracy: 0.7453 - val_loss: 0.7899 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 21/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.6943 - accuracy: 0.7392 - val_loss: 0.7266 - val_accuracy: 0.6928\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 22/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.6432 - accuracy: 0.7492 - val_loss: 0.8201 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 23/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.6158 - accuracy: 0.7545 - val_loss: 0.7439 - val_accuracy: 0.6754\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 24/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.6207 - accuracy: 0.7671 - val_loss: 0.6798 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 25/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.6139 - accuracy: 0.7593 - val_loss: 0.7082 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 26/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.6157 - accuracy: 0.7558 - val_loss: 0.7213 - val_accuracy: 0.6789\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 27/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.6416 - accuracy: 0.7562 - val_loss: 0.7029 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 28/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.6038 - accuracy: 0.7593 - val_loss: 0.7273 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.025.\n",
      "Epoch 29/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5956 - accuracy: 0.7479 - val_loss: 0.7670 - val_accuracy: 0.6597\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 30/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5798 - accuracy: 0.7762 - val_loss: 0.7263 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 31/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5723 - accuracy: 0.7653 - val_loss: 0.7036 - val_accuracy: 0.6963\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 32/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5765 - accuracy: 0.7771 - val_loss: 0.7404 - val_accuracy: 0.6736\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 33/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5656 - accuracy: 0.7771 - val_loss: 0.6675 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 34/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5593 - accuracy: 0.7893 - val_loss: 0.6465 - val_accuracy: 0.7312\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 35/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5730 - accuracy: 0.7762 - val_loss: 0.6613 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 36/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5769 - accuracy: 0.7741 - val_loss: 0.7048 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 37/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5876 - accuracy: 0.7741 - val_loss: 0.6911 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 38/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5915 - accuracy: 0.7623 - val_loss: 0.6966 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "Epoch 39/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5535 - accuracy: 0.7832 - val_loss: 0.6912 - val_accuracy: 0.6998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 40/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5443 - accuracy: 0.7867 - val_loss: 0.6959 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 41/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5411 - accuracy: 0.7780 - val_loss: 0.6653 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 42/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5686 - accuracy: 0.7767 - val_loss: 0.6944 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 43/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5419 - accuracy: 0.7819 - val_loss: 0.6884 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 44/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5401 - accuracy: 0.7797 - val_loss: 0.6740 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 45/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5580 - accuracy: 0.7671 - val_loss: 0.6695 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 46/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5604 - accuracy: 0.7784 - val_loss: 0.6779 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 47/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5411 - accuracy: 0.7954 - val_loss: 0.6675 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 48/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5607 - accuracy: 0.7767 - val_loss: 0.6202 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "Epoch 49/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5318 - accuracy: 0.7875 - val_loss: 0.6453 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 50/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5338 - accuracy: 0.7889 - val_loss: 0.6586 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 51/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5331 - accuracy: 0.7845 - val_loss: 0.6847 - val_accuracy: 0.6824\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 52/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5321 - accuracy: 0.7849 - val_loss: 0.6559 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 53/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5618 - accuracy: 0.7697 - val_loss: 0.6744 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 54/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5660 - accuracy: 0.7745 - val_loss: 0.6546 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 55/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5290 - accuracy: 0.7910 - val_loss: 0.6389 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 56/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5623 - accuracy: 0.7775 - val_loss: 0.6975 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 57/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5639 - accuracy: 0.7684 - val_loss: 0.6500 - val_accuracy: 0.7243\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 58/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5611 - accuracy: 0.7793 - val_loss: 0.6450 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "Epoch 59/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5382 - accuracy: 0.7858 - val_loss: 0.6511 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 60/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5388 - accuracy: 0.7819 - val_loss: 0.6699 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 61/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5267 - accuracy: 0.7806 - val_loss: 0.6607 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 62/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5508 - accuracy: 0.7810 - val_loss: 0.6569 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 63/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5387 - accuracy: 0.7858 - val_loss: 0.6779 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 64/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5497 - accuracy: 0.7801 - val_loss: 0.6344 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 65/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5542 - accuracy: 0.7793 - val_loss: 0.6595 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 66/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5636 - accuracy: 0.7675 - val_loss: 0.6419 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 67/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5582 - accuracy: 0.7793 - val_loss: 0.6291 - val_accuracy: 0.6963\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 68/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5448 - accuracy: 0.7775 - val_loss: 0.6275 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "Epoch 69/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5448 - accuracy: 0.7815 - val_loss: 0.6570 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 70/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5380 - accuracy: 0.7801 - val_loss: 0.6653 - val_accuracy: 0.6963\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 71/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5586 - accuracy: 0.7749 - val_loss: 0.6565 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 72/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5350 - accuracy: 0.7849 - val_loss: 0.6476 - val_accuracy: 0.7260\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 73/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5260 - accuracy: 0.7902 - val_loss: 0.6572 - val_accuracy: 0.6928\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 74/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5530 - accuracy: 0.7823 - val_loss: 0.6559 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 75/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5589 - accuracy: 0.7754 - val_loss: 0.6583 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 76/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5529 - accuracy: 0.7832 - val_loss: 0.6073 - val_accuracy: 0.7225\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 77/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5460 - accuracy: 0.7706 - val_loss: 0.6541 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 78/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 85s 5s/step - loss: 0.5431 - accuracy: 0.7836 - val_loss: 0.6586 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "Epoch 79/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5312 - accuracy: 0.7858 - val_loss: 0.6664 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 80/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5335 - accuracy: 0.7893 - val_loss: 0.6400 - val_accuracy: 0.7208\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 81/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5415 - accuracy: 0.7819 - val_loss: 0.6254 - val_accuracy: 0.7208\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 82/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5407 - accuracy: 0.7797 - val_loss: 0.6152 - val_accuracy: 0.7260\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 83/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5454 - accuracy: 0.7788 - val_loss: 0.6575 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 84/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5074 - accuracy: 0.7984 - val_loss: 0.6938 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 85/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5323 - accuracy: 0.7841 - val_loss: 0.6560 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 86/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5416 - accuracy: 0.7788 - val_loss: 0.6796 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 87/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5253 - accuracy: 0.7910 - val_loss: 0.6413 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 88/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5223 - accuracy: 0.7867 - val_loss: 0.6355 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "Epoch 89/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5253 - accuracy: 0.7858 - val_loss: 0.6828 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 90/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5384 - accuracy: 0.7832 - val_loss: 0.6549 - val_accuracy: 0.6963\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 91/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5306 - accuracy: 0.7815 - val_loss: 0.6724 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 92/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5287 - accuracy: 0.7889 - val_loss: 0.6512 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 93/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5406 - accuracy: 0.7819 - val_loss: 0.6290 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 94/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5482 - accuracy: 0.7862 - val_loss: 0.6563 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 95/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5539 - accuracy: 0.7727 - val_loss: 0.6622 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 96/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5456 - accuracy: 0.7845 - val_loss: 0.6423 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 97/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5470 - accuracy: 0.7723 - val_loss: 0.7001 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 98/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5523 - accuracy: 0.7762 - val_loss: 0.6539 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.0001953125.\n",
      "Epoch 99/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5078 - accuracy: 0.7910 - val_loss: 0.6618 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 100/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5350 - accuracy: 0.7828 - val_loss: 0.6238 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 101/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5297 - accuracy: 0.7836 - val_loss: 0.6307 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 102/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5490 - accuracy: 0.7849 - val_loss: 0.6599 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 103/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5180 - accuracy: 0.7832 - val_loss: 0.6692 - val_accuracy: 0.6928\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 104/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5308 - accuracy: 0.7910 - val_loss: 0.6808 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 105/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5443 - accuracy: 0.7810 - val_loss: 0.6567 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 106/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5198 - accuracy: 0.7875 - val_loss: 0.6351 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 107/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5582 - accuracy: 0.7775 - val_loss: 0.6562 - val_accuracy: 0.6963\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 108/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5280 - accuracy: 0.7836 - val_loss: 0.6731 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 9.765625e-05.\n",
      "Epoch 109/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5463 - accuracy: 0.7771 - val_loss: 0.6494 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 110/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5569 - accuracy: 0.7758 - val_loss: 0.6640 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 111/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5292 - accuracy: 0.7902 - val_loss: 0.6419 - val_accuracy: 0.7243\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 112/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5564 - accuracy: 0.7801 - val_loss: 0.6705 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 113/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5556 - accuracy: 0.7780 - val_loss: 0.6542 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 114/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5404 - accuracy: 0.7780 - val_loss: 0.6800 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 115/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5303 - accuracy: 0.7915 - val_loss: 0.6612 - val_accuracy: 0.7138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 116/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5360 - accuracy: 0.7862 - val_loss: 0.6757 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 117/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5362 - accuracy: 0.7823 - val_loss: 0.6553 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 118/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5429 - accuracy: 0.7828 - val_loss: 0.6546 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 4.8828125e-05.\n",
      "Epoch 119/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5321 - accuracy: 0.7867 - val_loss: 0.6502 - val_accuracy: 0.6771\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 120/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5360 - accuracy: 0.7923 - val_loss: 0.6735 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 121/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5395 - accuracy: 0.7849 - val_loss: 0.6519 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 122/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5351 - accuracy: 0.7871 - val_loss: 0.6788 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 123/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5245 - accuracy: 0.7858 - val_loss: 0.6537 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 124/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5495 - accuracy: 0.7688 - val_loss: 0.6620 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 125/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5314 - accuracy: 0.7849 - val_loss: 0.6481 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 126/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5311 - accuracy: 0.7858 - val_loss: 0.6675 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 127/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5147 - accuracy: 0.7936 - val_loss: 0.6449 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 128/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5262 - accuracy: 0.7889 - val_loss: 0.6324 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 2.44140625e-05.\n",
      "Epoch 129/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5457 - accuracy: 0.7745 - val_loss: 0.6702 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 130/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5236 - accuracy: 0.7793 - val_loss: 0.6497 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 131/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5295 - accuracy: 0.7910 - val_loss: 0.6526 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 132/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5341 - accuracy: 0.7723 - val_loss: 0.7176 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 133/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5155 - accuracy: 0.7845 - val_loss: 0.6585 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 134/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5438 - accuracy: 0.7858 - val_loss: 0.6454 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 135/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5511 - accuracy: 0.7780 - val_loss: 0.6236 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 136/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5397 - accuracy: 0.7849 - val_loss: 0.6748 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 137/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5337 - accuracy: 0.7841 - val_loss: 0.6542 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 138/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5490 - accuracy: 0.7784 - val_loss: 0.6450 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 1.220703125e-05.\n",
      "Epoch 139/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5090 - accuracy: 0.8015 - val_loss: 0.7039 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 140/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5411 - accuracy: 0.7775 - val_loss: 0.6419 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 141/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5289 - accuracy: 0.7875 - val_loss: 0.6586 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 142/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5567 - accuracy: 0.7828 - val_loss: 0.6563 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 143/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5497 - accuracy: 0.7749 - val_loss: 0.6563 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 144/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5606 - accuracy: 0.7771 - val_loss: 0.6533 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 145/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5367 - accuracy: 0.7845 - val_loss: 0.6593 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 146/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5168 - accuracy: 0.7871 - val_loss: 0.6212 - val_accuracy: 0.7312\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 147/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5334 - accuracy: 0.7889 - val_loss: 0.6564 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 148/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5300 - accuracy: 0.7875 - val_loss: 0.6176 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 6.103515625e-06.\n",
      "Epoch 149/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5389 - accuracy: 0.7871 - val_loss: 0.6655 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 150/250\n",
      "18/18 [==============================] - 87s 5s/step - loss: 0.5210 - accuracy: 0.7858 - val_loss: 0.6678 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 151/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5229 - accuracy: 0.7980 - val_loss: 0.6544 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 152/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5378 - accuracy: 0.7884 - val_loss: 0.6302 - val_accuracy: 0.7330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 153/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5382 - accuracy: 0.7819 - val_loss: 0.6838 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 154/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5228 - accuracy: 0.7915 - val_loss: 0.6605 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 155/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5184 - accuracy: 0.7984 - val_loss: 0.6249 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 156/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5482 - accuracy: 0.7771 - val_loss: 0.6753 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 157/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5255 - accuracy: 0.7910 - val_loss: 0.6534 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 158/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5615 - accuracy: 0.7801 - val_loss: 0.6578 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 3.0517578125e-06.\n",
      "Epoch 159/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5537 - accuracy: 0.7749 - val_loss: 0.6337 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 160/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5405 - accuracy: 0.7819 - val_loss: 0.6793 - val_accuracy: 0.6963\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 161/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5361 - accuracy: 0.7815 - val_loss: 0.6828 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 162/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5311 - accuracy: 0.7845 - val_loss: 0.6413 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 163/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5104 - accuracy: 0.7919 - val_loss: 0.6450 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 164/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5326 - accuracy: 0.7867 - val_loss: 0.6358 - val_accuracy: 0.7208\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 165/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5289 - accuracy: 0.7906 - val_loss: 0.6243 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 166/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5310 - accuracy: 0.7893 - val_loss: 0.6517 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 167/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5427 - accuracy: 0.7854 - val_loss: 0.6751 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 168/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5397 - accuracy: 0.7754 - val_loss: 0.6368 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 1.52587890625e-06.\n",
      "Epoch 169/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5423 - accuracy: 0.7732 - val_loss: 0.6481 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 170/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5362 - accuracy: 0.7923 - val_loss: 0.6759 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 171/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5421 - accuracy: 0.7871 - val_loss: 0.6810 - val_accuracy: 0.6928\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 172/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5348 - accuracy: 0.7741 - val_loss: 0.6424 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 173/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5311 - accuracy: 0.7893 - val_loss: 0.6588 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 174/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5216 - accuracy: 0.7928 - val_loss: 0.6390 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 175/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5428 - accuracy: 0.7875 - val_loss: 0.6750 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 176/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5517 - accuracy: 0.7767 - val_loss: 0.6356 - val_accuracy: 0.7347\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 177/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5346 - accuracy: 0.7932 - val_loss: 0.6750 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 178/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5473 - accuracy: 0.7810 - val_loss: 0.6567 - val_accuracy: 0.7225\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 7.62939453125e-07.\n",
      "Epoch 179/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5355 - accuracy: 0.7793 - val_loss: 0.6583 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 180/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5443 - accuracy: 0.7810 - val_loss: 0.6744 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 181/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5216 - accuracy: 0.7954 - val_loss: 0.6763 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 182/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5414 - accuracy: 0.7862 - val_loss: 0.6588 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 183/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5331 - accuracy: 0.7754 - val_loss: 0.6321 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 184/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5250 - accuracy: 0.7854 - val_loss: 0.6392 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 185/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5355 - accuracy: 0.7893 - val_loss: 0.6622 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 186/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5504 - accuracy: 0.7741 - val_loss: 0.6901 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 187/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5579 - accuracy: 0.7801 - val_loss: 0.6430 - val_accuracy: 0.6771\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 188/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5367 - accuracy: 0.7915 - val_loss: 0.6832 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 3.814697265625e-07.\n",
      "Epoch 189/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5435 - accuracy: 0.7871 - val_loss: 0.6830 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 190/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5444 - accuracy: 0.7754 - val_loss: 0.6288 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 191/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5291 - accuracy: 0.7867 - val_loss: 0.6520 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 192/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5456 - accuracy: 0.7849 - val_loss: 0.6576 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 193/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5321 - accuracy: 0.7832 - val_loss: 0.6788 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 194/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5531 - accuracy: 0.7810 - val_loss: 0.6698 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 195/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5214 - accuracy: 0.7893 - val_loss: 0.6684 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 196/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5228 - accuracy: 0.7910 - val_loss: 0.6706 - val_accuracy: 0.7051\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 197/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5547 - accuracy: 0.7714 - val_loss: 0.6507 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 198/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5508 - accuracy: 0.7745 - val_loss: 0.6358 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 1.9073486328125e-07.\n",
      "Epoch 199/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5324 - accuracy: 0.7963 - val_loss: 0.6578 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 200/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5243 - accuracy: 0.7849 - val_loss: 0.6822 - val_accuracy: 0.6771\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 201/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5029 - accuracy: 0.7893 - val_loss: 0.6525 - val_accuracy: 0.7103\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 202/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5548 - accuracy: 0.7745 - val_loss: 0.6298 - val_accuracy: 0.7312\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 203/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5468 - accuracy: 0.7788 - val_loss: 0.6834 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 204/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5634 - accuracy: 0.7788 - val_loss: 0.6642 - val_accuracy: 0.6824\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 205/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5252 - accuracy: 0.7928 - val_loss: 0.6590 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 206/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5417 - accuracy: 0.7841 - val_loss: 0.6536 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 207/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5528 - accuracy: 0.7749 - val_loss: 0.6729 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 208/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5187 - accuracy: 0.7880 - val_loss: 0.6742 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 9.5367431640625e-08.\n",
      "Epoch 209/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5324 - accuracy: 0.7845 - val_loss: 0.6343 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 210/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5634 - accuracy: 0.7801 - val_loss: 0.6646 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 211/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5345 - accuracy: 0.7858 - val_loss: 0.6721 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 212/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5222 - accuracy: 0.7910 - val_loss: 0.6763 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 213/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5285 - accuracy: 0.7932 - val_loss: 0.6435 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 214/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5480 - accuracy: 0.7736 - val_loss: 0.6536 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 215/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5179 - accuracy: 0.7915 - val_loss: 0.6730 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 216/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5369 - accuracy: 0.7836 - val_loss: 0.6627 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 217/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5311 - accuracy: 0.7854 - val_loss: 0.6598 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 218/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5402 - accuracy: 0.7875 - val_loss: 0.6652 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 4.76837158203125e-08.\n",
      "Epoch 219/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5219 - accuracy: 0.7849 - val_loss: 0.6705 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 220/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5166 - accuracy: 0.7936 - val_loss: 0.6637 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 221/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5336 - accuracy: 0.7749 - val_loss: 0.6367 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 222/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5184 - accuracy: 0.7936 - val_loss: 0.6943 - val_accuracy: 0.6876\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 223/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5534 - accuracy: 0.7771 - val_loss: 0.6704 - val_accuracy: 0.6894\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 224/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5279 - accuracy: 0.7749 - val_loss: 0.6172 - val_accuracy: 0.7347\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 225/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 85s 5s/step - loss: 0.5350 - accuracy: 0.7823 - val_loss: 0.6690 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 226/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5476 - accuracy: 0.7758 - val_loss: 0.6366 - val_accuracy: 0.7382\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 227/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5230 - accuracy: 0.7932 - val_loss: 0.6398 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 228/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5210 - accuracy: 0.7923 - val_loss: 0.6722 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 2.384185791015625e-08.\n",
      "Epoch 229/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5531 - accuracy: 0.7793 - val_loss: 0.6423 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 230/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5318 - accuracy: 0.7889 - val_loss: 0.6474 - val_accuracy: 0.6981\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 231/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5487 - accuracy: 0.7771 - val_loss: 0.6822 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 232/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5358 - accuracy: 0.7819 - val_loss: 0.6573 - val_accuracy: 0.7190\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 233/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5400 - accuracy: 0.7819 - val_loss: 0.6432 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 234/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5535 - accuracy: 0.7732 - val_loss: 0.6093 - val_accuracy: 0.7452\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 235/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5048 - accuracy: 0.7889 - val_loss: 0.6544 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 236/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5329 - accuracy: 0.7832 - val_loss: 0.6868 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 237/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5594 - accuracy: 0.7775 - val_loss: 0.6930 - val_accuracy: 0.6859\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 238/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5353 - accuracy: 0.7727 - val_loss: 0.6609 - val_accuracy: 0.7016\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 1.1920928955078126e-08.\n",
      "Epoch 239/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5578 - accuracy: 0.7758 - val_loss: 0.7149 - val_accuracy: 0.6911\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 240/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5702 - accuracy: 0.7758 - val_loss: 0.6336 - val_accuracy: 0.7173\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 241/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5522 - accuracy: 0.7762 - val_loss: 0.6895 - val_accuracy: 0.6963\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 242/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5529 - accuracy: 0.7889 - val_loss: 0.6323 - val_accuracy: 0.7086\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 243/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5213 - accuracy: 0.7928 - val_loss: 0.6629 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 244/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5274 - accuracy: 0.7858 - val_loss: 0.6942 - val_accuracy: 0.6998\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 245/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5520 - accuracy: 0.7780 - val_loss: 0.6387 - val_accuracy: 0.7068\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 246/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5204 - accuracy: 0.7871 - val_loss: 0.6542 - val_accuracy: 0.7155\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 247/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5259 - accuracy: 0.7880 - val_loss: 0.6397 - val_accuracy: 0.7138\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 248/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5572 - accuracy: 0.7662 - val_loss: 0.6571 - val_accuracy: 0.6946\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 5.960464477539063e-09.\n",
      "Epoch 249/250\n",
      "18/18 [==============================] - 85s 5s/step - loss: 0.5129 - accuracy: 0.7980 - val_loss: 0.6445 - val_accuracy: 0.7243\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 2.9802322387695314e-09.\n",
      "Epoch 250/250\n",
      "18/18 [==============================] - 86s 5s/step - loss: 0.5515 - accuracy: 0.7801 - val_loss: 0.7006 - val_accuracy: 0.6736\n"
     ]
    }
   ],
   "source": [
    "model = create_nasnet_model()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    workers=6,\n",
    "    max_queue_size=100,\n",
    "    verbose=True,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "NASNet (Functional)          (None, 7, 7, 1056)        4269716   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1056)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 4228      \n",
      "=================================================================\n",
      "Total params: 4,273,944\n",
      "Trainable params: 4,228\n",
      "Non-trainable params: 4,269,716\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 7s 2s/step - loss: 1.2307 - accuracy: 0.6142\n",
      "Test Accuracy: 61.42131686210632%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_generator)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211) \n",
    "plt.plot(history.history['accuracy']) \n",
    "plt.plot(history.history['val_accuracy']) \n",
    "plt.title('model accuracy') \n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Accuracy', 'val_acc'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(212) \n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout() \n",
    "plt.savefig('acc_loss_50.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
